{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c520d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (4.15.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\yodchan\\appdata\\local\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import json, os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc4135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## instantiate driver\n",
    "## check the version of Google Chrome and download correct version of chromedriver\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fec790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get page of \"social grep\", which gived old posts of subreddit\n",
    "## e.g. https://socialgrep.com/search?query=%2Fr%2FLanguageTechnology%2Cafter%3A2010-01-01&order_by=oldest\n",
    "## original reddit url = 'https://www.reddit.com/r/xxxxxxxxx/'\n",
    "\n",
    "subreddit = 'BMW' # choose by yourself\n",
    "start_date = '2010-01-01' # choose by yourself\n",
    "\n",
    "url = f'https://socialgrep.com/search?query=%2Fr%2F{subreddit}%2Cafter%3A{start_date}&order_by=oldest'\n",
    "\n",
    "driver.get(url)\n",
    "repeat_time, waiting_time = 4, 2\n",
    "\n",
    "## scroll to the bottom of the page and wait\n",
    "for i in range(repeat_time):\n",
    "    driver.execute_script(f\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    sleep(waiting_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b0fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to scrape\n",
    "def get_content(post, subreddit):\n",
    "    try:\n",
    "        vote = int(post.select_one('span.text-info').text)\n",
    "    except:\n",
    "        vote = 0\n",
    "    try:\n",
    "        title = post.a.text\n",
    "    except:\n",
    "        return None\n",
    "    try:\n",
    "        text = post.select_one('div.post_content').get_text(separator='\\n').strip()\n",
    "        if text == '':\n",
    "            text = None\n",
    "    except:\n",
    "        text = None\n",
    "    date = post.select_one('h6.card-subtitle').text.split(',')[1].strip()\n",
    "\n",
    "    if text == None and title == f\"/r/{subreddit.lower()}\":\n",
    "        return None\n",
    "    else:\n",
    "        return {\n",
    "            \"vote\" : vote,\n",
    "            \"title\" : title,\n",
    "            \"text\" : text,\n",
    "            \"date\" : date\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc907724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vote': 9,\n",
       " 'title': 'V10 E30 smokes a V8 M3',\n",
       " 'text': None,\n",
       " 'date': '2010-01-08'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(driver.page_source)\n",
    "posts = soup.select('div.card-body') # content is under here\n",
    "\n",
    "get_content(posts[1], subreddit) # show one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b98065",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{subreddit}.json'):\n",
    "    ## resume scraping from the last date in the json file\n",
    "    with open(f'{subreddit}.json', 'r', encoding = 'utf8') as f:\n",
    "        scraped_data = json.load(f)\n",
    "    new_date = scraped_data[-1]['date']\n",
    "    url = f'https://socialgrep.com/search?query=%2Fr%2F{subreddit}%2Cafter%3A{new_date}&order_by=oldest'\n",
    "else:\n",
    "    ## if the file not exists, create a new list\n",
    "    scraped_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3838d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–Œ                                                                            | 27/3864 [04:20<10:17:38,  9.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m      9\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     sleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m## get HTML\u001b[39;00m\n\u001b[0;32m     13\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## scrape and append to `scraped_data`\n",
    "## RUN THIS CELL AGAIN AND AGAIN until getting the latest post\n",
    "\n",
    "for _ in tqdm(range(3864)): # set repeat time \n",
    "\n",
    "    ## scroll to the bottom of the page and wait\n",
    "    driver.get(url)\n",
    "    for i in range(4):\n",
    "        driver.execute_script(f\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        sleep(2)\n",
    "\n",
    "    ## get HTML\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    posts = soup.select('div.card-body')\n",
    "\n",
    "    ## iterate each post\n",
    "    for post in posts:\n",
    "        one_post_dict = get_content(post, subreddit)\n",
    "        if one_post_dict != None:\n",
    "            scraped_data.append(one_post_dict)\n",
    "\n",
    "    ## save to json\n",
    "    with open(f'{subreddit}.json', 'w', encoding ='utf8') as f:\n",
    "        json.dump(scraped_data, f, indent=False, ensure_ascii=False)\n",
    "\n",
    "    ## set new date\n",
    "    new_date = scraped_data[-1]['date']\n",
    "    url = f'https://socialgrep.com/search?query=%2Fr%2F{subreddit}%2Cafter%3A{new_date}&order_by=oldest'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e3f6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vote</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>The only word for this is epic - V10 swapped i...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2010-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>V10 E30 smokes a V8 M3</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>BMW three-turbo diesel engine</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BMW M Sauber</td>\n",
       "      <td>Drop that F1-inspired 500 horsepower V10 into ...</td>\n",
       "      <td>2010-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Anyone here got a 135i?</td>\n",
       "      <td>I'm thinking of purchasing one in the next few...</td>\n",
       "      <td>2010-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31472</th>\n",
       "      <td>5</td>\n",
       "      <td>Since Front End Friday is a thing, I guess it'...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31473</th>\n",
       "      <td>14</td>\n",
       "      <td>One week after delivery very impressed with th...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31474</th>\n",
       "      <td>31</td>\n",
       "      <td>Frontend Friday</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31475</th>\n",
       "      <td>8</td>\n",
       "      <td>Garaged the E36, 335xi gets Winter Beater status</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31476</th>\n",
       "      <td>11</td>\n",
       "      <td>Front end friday</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31435 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vote                                              title  \\\n",
       "0         6  The only word for this is epic - V10 swapped i...   \n",
       "1         9                             V10 E30 smokes a V8 M3   \n",
       "2         8                      BMW three-turbo diesel engine   \n",
       "3         3                                       BMW M Sauber   \n",
       "4         3                            Anyone here got a 135i?   \n",
       "...     ...                                                ...   \n",
       "31472     5  Since Front End Friday is a thing, I guess it'...   \n",
       "31473    14  One week after delivery very impressed with th...   \n",
       "31474    31                                    Frontend Friday   \n",
       "31475     8   Garaged the E36, 335xi gets Winter Beater status   \n",
       "31476    11                                   Front end friday   \n",
       "\n",
       "                                                    text       date  \n",
       "0                                              [deleted] 2010-01-07  \n",
       "1                                                   None 2010-01-08  \n",
       "2                                                   None 2010-01-18  \n",
       "3      Drop that F1-inspired 500 horsepower V10 into ... 2010-01-31  \n",
       "4      I'm thinking of purchasing one in the next few... 2010-02-11  \n",
       "...                                                  ...        ...  \n",
       "31472                                               None 2018-11-30  \n",
       "31473                                               None 2018-11-30  \n",
       "31474                                               None 2018-11-30  \n",
       "31475                                          [deleted] 2018-11-30  \n",
       "31476                                               None 2018-11-30  \n",
       "\n",
       "[31435 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(f'{subreddit}.json').drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ad410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vote         0\n",
       "title        0\n",
       "text     17026\n",
       "date         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## missing value in text\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf9f7502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vote</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>The only word for this is epic - V10 swapped i...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2010-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>V10 E30 smokes a V8 M3</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>BMW three-turbo diesel engine</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>BMW.co.uk Blog - The new X5</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-02-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Fellow member launches Euro Car site with Give...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2010-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31472</th>\n",
       "      <td>5</td>\n",
       "      <td>Since Front End Friday is a thing, I guess it'...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31473</th>\n",
       "      <td>14</td>\n",
       "      <td>One week after delivery very impressed with th...</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31474</th>\n",
       "      <td>31</td>\n",
       "      <td>Frontend Friday</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31475</th>\n",
       "      <td>8</td>\n",
       "      <td>Garaged the E36, 335xi gets Winter Beater status</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31476</th>\n",
       "      <td>11</td>\n",
       "      <td>Front end friday</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-11-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24680 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vote                                              title       text  \\\n",
       "0         6  The only word for this is epic - V10 swapped i...  [deleted]   \n",
       "1         9                             V10 E30 smokes a V8 M3       None   \n",
       "2         8                      BMW three-turbo diesel engine       None   \n",
       "5         0                        BMW.co.uk Blog - The new X5       None   \n",
       "6         2  Fellow member launches Euro Car site with Give...  [deleted]   \n",
       "...     ...                                                ...        ...   \n",
       "31472     5  Since Front End Friday is a thing, I guess it'...       None   \n",
       "31473    14  One week after delivery very impressed with th...       None   \n",
       "31474    31                                    Frontend Friday       None   \n",
       "31475     8   Garaged the E36, 335xi gets Winter Beater status  [deleted]   \n",
       "31476    11                                   Front end friday       None   \n",
       "\n",
       "            date  \n",
       "0     2010-01-07  \n",
       "1     2010-01-08  \n",
       "2     2010-01-18  \n",
       "5     2010-02-15  \n",
       "6     2010-02-18  \n",
       "...          ...  \n",
       "31472 2018-11-30  \n",
       "31473 2018-11-30  \n",
       "31474 2018-11-30  \n",
       "31475 2018-11-30  \n",
       "31476 2018-11-30  \n",
       "\n",
       "[24680 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## text includes [removed] [deleted]\n",
    "df[df['text'].isin(['[removed]', '[deleted]',None,'',np.NaN])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515cf825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742a3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96cc61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
